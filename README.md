# tec-NN
TEC-1 Neural Network

Explore a range of code written in MINT to perform any of the NN operations for experimentation. I like point 6 and 20.

### def
"neural networks, are computing systems inspired by the biological neural networks that constitute animal brains. An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain."
"In the case of recognizing handwriting or facial recognition, the brain very quickly makes some decisions."
"There are several kinds of artificial neural networks. These types of networks are implemented based on the mathematical operations and a set of parameters required to determine the output."

### 20 neural network types, and there are many more

1. Feedforward neural network (FFN), uses perceptron algorithm 
2. Convolutional neural network, uses deep learning algorithm
3. Recurrent neural network, "are the state of the art algorithm for sequential data and are used by Apple's Siri and and Google's voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data."
4. Long short-term memory, "are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. This is a behavior required in complex problem domains like machine translation, speech recognition, and more. LSTMs are a complex area of deep learning."
5. Gated recurrent unit, "are a gating mechanism in recurrent neural networks, use connections through a sequence of nodes to perform machine learning tasks associated with memory and clustering, for instance, in speech recognition."
6. Echo state network, "is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns." see https://github.com/SteveJustin1963/tec-BRAIN
7. Self-organizing map, "is an unsupervised machine learning technique used to produce a low-dimensional representation of a higher dimensional data set while preserving the topological structure of the data."
8. Neural Turing machine, "is a recurrent neural network model of a Turing machine, its a neural network that can learn to read and write from an external memory, just like a Turing machine. This makes it much more powerful than a traditional neural network, which can only learn to recognize patterns that are present in its training data."
9. Deep belief network, "is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables, with connections between the layers but not between units within each layer."
10. Stacked autoencoder, "is a neural network which is an unsupervised learning algorithm which uses back propagation to generate output value which is almost close to the input value. Iis a type of neural network that uses multiple layers to learn to represent data in a more efficient way. The first layer of a stacked autoencoder learns to represent the input data, and each subsequent layer learns to represent the data that was learned by the previous layer. The final layer of a stacked autoencoder is a decoding layer that takes the learned representation and reconstructs the original input data."
11. Generative adversarial network, " are a clever way of training a generative model by framing the problem as a supervised learning problem with two sub-models: the generator model that we train to generate new examples, and the discriminator model that tries to classify examples as either real (from the domain) or fake (generated). The two models are trained together in a zero-sum game, adversarial, until the discriminator model is fooled about half the time, meaning the generator model is generating plausible examples."
12. Siamese network,  A Siamese neural network is a type of neural network that uses two or more identical subnetworks to process information. The subnetworks share weights and biases, and are typically used to compare two input vectors to see if they are similar.. "is an artificial neural network that uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. Often one of the output vectors is precomputed, thus forming a baseline against which the other output vector is compared. This is similar to comparing fingerprints but can be described more technically as a distance function for locality-sensitive hashing."
13. Highway network, "The Network Design Problem (NDP) has long been recognized to be one of ... Models and algorithms for road network design..." " is a neural network algorithm that allows for the training of deep networks. The algorithm is based on the idea of training a network in a way that allows for the efficient propagation of information through the network. The highway network algorithm was proposed by Rupesh Kumar Srivastava, et al. in their 2015 paper "Highway Networks"."
14. Dropout,  Dropout is a technique for reducing overfitting in neural networks by randomly setting a number of output units in the network to zero during the forward pass. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it averages the predictions of all these thinned networks.
15. Perceptron, The perceptron algorithm is a linear classifier. It is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. The algorithm is inspired by the structure of the human brain, specifically the neural networks that make up the brain. The algorithm is also sometimes referred to as the artificial neuron or the single-layer perceptron. The perceptron algorithm was developed in the 1950s by Frank Rosenblatt. It is a relatively simple algorithm and can be trained on a small amount of data. The algorithm is not very accurate, but it is still used in some applications where accuracy is not the most important concern.
16. Multilayer perceptron, 
Multilayer perceptron (MLP) is a supervised learning algorithm that learns a function by training on a dataset, where the data is presented in arrays of input values x and target values t. The algorithm consists of a series of mathematical operations that are executed on the input values to produce the target values. The input values are multiplied by a weight matrix, and then the results are passed through a nonlinear activation function. The output of the activation function is then multiplied by another weight matrix, and the results are passed through another activation function. This process is repeated until the final output is produced.
17. Radial basis function network, is a type of neural network that uses radial basis functions as its activation function. Radial basis functions are a type of activation function that are used in a number of different neural network architectures, including the popular backpropagation algorithm.
18. Support vector machine, is a supervised machine learning algorithm that can be used for both classification and regression tasks. The algorithm is a discriminative classifier that attempts to draw a line between two clusters of data. In linear SVM classification, the line is drawn so that it maximally separates the two groups of data. In SVM regression, the line is drawn so as to minimize the error between the predicted values and the actual values.
19. Max-pooling, Max pooling is a type of pooling layer used in convolutional neural networks. A pooling layer performs a down-sampling operation on the input, reducing its size and dimensionality. The max pooling layer operates on a window of n x n pixels, and a stride of s. It outputs the maximum value of the n x n window for each s pixels. For example, if the input is an image with dimensions 4 x 4, and the max pooling layer has a window of size 2 x 2 and a stride of 2, the output will be an image with dimensions 2 x 2.
20. Average-pooling, The average pooling operation is a way of downsampling the feature map. This is done by taking the average value for each patch of the feature map. This means that each NxN square of the feature map is down sampled to the average value in that NxN square. This can be seen as a reduction in the dimensionality of the feature map.


### iterate
- https://github.com/SteveJustin1963/tec-AI
- https://github.com/SteveJustin1963/tec-memR
- https://github.com/SteveJustin1963/tec-Generative-Adversarial-Network
- https://github.com/SteveJustin1963/tec-BOT
- https://github.com/SteveJustin1963/tec-BRAIN
- https://github.com/SteveJustin1963/tec-GA
- 



### Ref
- https://en.wikipedia.org/wiki/Neural_network
- https://www.investopedia.com/terms/n/neuralnetwork.asp#:~:text=A%20neural%20network%20is%20a,organic%20or%20artificial%20in%20nature.
- https://www.ibm.com/cloud/learn/neural-networks
- https://analyticsindiamag.com/6-types-of-artificial-neural-networks-currently-being-used-in-todays-technology/
- https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/#:~:text=Average%20pooling%20involves%20calculating%20the,6%C3%976%20feature%20map.
- https://dzone.com/articles/the-very-basic-introduction-to-feed-forward-neural
- 
